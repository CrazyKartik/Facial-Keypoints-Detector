{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Facial Keypoints Detection(Kaggle).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "gPqVuP3ZTc-x",
        "kNqI8v4sP6WD",
        "69sJ_UmFAPpi",
        "fzyd2ENpGTpr",
        "HldNACWbhVAs",
        "-txBDmZXUtaE",
        "E9DdrbyX3a_k",
        "ROtLIafmWEvE",
        "-EH2jEDnbnxH",
        "ZTebQapbmc95",
        "FOpYm3p9T3UU",
        "hrmwK-enWtQD",
        "dgl7gN4OX94v",
        "oQAKh4JzJjjG",
        "7sS1tGzPW6tj",
        "wBDWMc9kXLvx",
        "76J-KrmYZZx1",
        "ouo99TxItsbG",
        "eLsRvQF_4JV1"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPqVuP3ZTc-x",
        "colab_type": "text"
      },
      "source": [
        "###**Connecting to Kaggle and loading data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hIucwb_YTbYe",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqg_vDl7mIi_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXpFjRNSmUQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf4FYxdsmZRs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTQLPBcNp3I0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kaggle competitions download -c facial-keypoints-detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XmHEnrbqEwZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip \\test.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2HDbEk9qOGR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip \\training.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNqI8v4sP6WD",
        "colab_type": "text"
      },
      "source": [
        "### **Importing Necessary Packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzXavs81rXkD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import utilities as utils\n",
        "import DataAugmentation\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from sklearn.utils import shuffle\n",
        "from PIL import Image, ImageFile\n",
        "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69sJ_UmFAPpi",
        "colab_type": "text"
      },
      "source": [
        "###**Cleaning, Normalizing and Loading Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dl0JJsJhDBpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, y_train = utils.loadData('train', drop_null=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFidpquODXH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (\"Shape of X:\",X_train.shape, \"Min of X:\", X_train.min(), \"Max of X:\", X_train.max())\n",
        "print (\"Shape of y:\",y_train.shape, \"Min of y:\", y_train.min(), \"Max of y:\", y_train.max())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIlApfIxIYuN",
        "colab_type": "text"
      },
      "source": [
        "**Exploring Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOgsgnzjGSpo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(10,10))\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.1, wspace=0.1)\n",
        "\n",
        "images = np.random.choice(range(X_train.shape[0]), 16, replace=False)\n",
        "for i in range(16):\n",
        "  axis = fig.add_subplot(5,4, i+1, xticks=[], yticks=[])\n",
        "  utils.plotSample(X_train[i], y_train[i], axis)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzyd2ENpGTpr",
        "colab_type": "text"
      },
      "source": [
        "### **Model 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HldNACWbhVAs",
        "colab_type": "text"
      },
      "source": [
        "####**Defining Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucv4zbicGVzp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Model_1():\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2D(filters=32,\n",
        "                                   kernel_size=3, \n",
        "                                   activation='relu', \n",
        "                                   input_shape=(96,96,1)\n",
        "                                   ))\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2D(filters=128,\n",
        "                                   kernel_size=4,\n",
        "                                   activation='relu',\n",
        "                                   ))\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "  model.add(tf.keras.layers.Dropout(0.1))\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2D(filters=256,\n",
        "                                   kernel_size=5,\n",
        "                                   activation='relu',\n",
        "                                   ))\n",
        "  # model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(3,3)))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "#   model.add(tf.keras.layers.Dropout(0.1))\n",
        "  model.add(tf.keras.layers.Dense(30))\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guCGIZhvtXqT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model_1()\n",
        "base_learning_rate = 1e-4\n",
        "\n",
        "model.compile(\n",
        "    loss = 'mse',\n",
        "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate),\n",
        "    metrics = ['accuracy'] \n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VPWAl_nUZRL",
        "colab_type": "text"
      },
      "source": [
        "**Model Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqVLAkOBvQMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_model(\n",
        "    model, \n",
        "    to_file='model.png', \n",
        "    show_layer_names=True, \n",
        "    show_shapes=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-txBDmZXUtaE",
        "colab_type": "text"
      },
      "source": [
        "####**Training and Checking the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynCYeMplwA4k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 300 #epochs before the loss starts increasing\n",
        "batch_size = 32\n",
        "\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fethU9a6QQiR",
        "colab_type": "text"
      },
      "source": [
        "**Learning Rate Check**\n",
        "A check for the optimal curve, monotonically decreasing loss without regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxkLX3DjQPiL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "plt.plot(loss,linewidth=2, label=\"training loss\")\n",
        "plt.plot(val_loss, linewidth=2, label=\"validation loss\")\n",
        "\n",
        "plt.legend()\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"log loss\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(acc, linewidth=2, label='training accuracy')\n",
        "plt.plot(val_acc, linewidth=2, label='validation accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psVMz1qYU6xK",
        "colab_type": "text"
      },
      "source": [
        "**Continue training with new learning rate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iG5LqnrdMxLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "more_epochs = 500\n",
        "new_learning_rate = 1e-6\n",
        "\n",
        "model.compile(\n",
        "    loss = 'mse',\n",
        "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=new_learning_rate),\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "last_epoch = history.epoch[-1] + 1\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=more_epochs+last_epoch,\n",
        "    initial_epoch = last_epoch,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8_cpahu0T8y",
        "colab_type": "text"
      },
      "source": [
        "**Learning Rate Check** A check for the optimal curve, monotonically decreasing loss without regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXLBu-cf0WEw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "plt.plot(loss,linewidth=2, label=\"training loss\")\n",
        "plt.plot(val_loss, linewidth=2, label=\"validation loss\")\n",
        "\n",
        "plt.legend()\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"log loss\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(acc, linewidth=2, label='training accuracy')\n",
        "plt.plot(val_acc, linewidth=2, label='validation accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9DdrbyX3a_k",
        "colab_type": "text"
      },
      "source": [
        "####**Testing the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0SjYSY43dk4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test, _ = utils.loadData()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV3120za4NrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnPO0ug75tcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(10,10))\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.3, wspace=0.1)\n",
        "\n",
        "images = np.random.choice(range(X_test.shape[0]), 16, replace=False)\n",
        "\n",
        "for i in range(16):\n",
        "  axis = fig.add_subplot(5,4, i+1, xticks=[], yticks=[])\n",
        "  utils.plotSample(X_test[images[i]], y_test[images[i]], axis, \"Test Prediction {}\".format(i+1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROtLIafmWEvE",
        "colab_type": "text"
      },
      "source": [
        "###**Adding Data Augmentation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EH2jEDnbnxH",
        "colab_type": "text"
      },
      "source": [
        "#####**Preparing Augmented Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llx7hyqhWJQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "augmentor = DataAugmentation.Augmentor()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnRM-hVlciJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "h_flipped_images, h_flipped_keypoints = augmentor.flip(X_train, y_train)\n",
        "v_flipped_images, v_flipped_keypoints = augmentor.flip(X_train, y_train, False)\n",
        "h_flipped_images, h_flipped_keypoints = shuffle(h_flipped_images, h_flipped_keypoints, random_state=2)\n",
        "v_flipped_images, v_flipped_keypoints = shuffle(v_flipped_images, v_flipped_keypoints, random_state=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCmQsUlwczfS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rotation_angles = [12]\n",
        "rotated_images, rotated_keypoints = augmentor.rotate(X_train, y_train, rotation_angles)\n",
        "rotated_images, rotated_keypoints = shuffle(rotated_images, rotated_keypoints, random_state=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqrZaGe-dBzq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "altered_images, altered_keypoints = augmentor.alterBrightness(rotated_images, rotated_keypoints, B=True, D=True)\n",
        "altered_images, altered_keypoints = shuffle(altered_images, altered_keypoints, random_state=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_jrbcDOddcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shifts = [12]\n",
        "shifted_images, shifted_keypoints = augmentor.shiftImage(altered_images, altered_keypoints, shifts=shifts)\n",
        "shifted_images, shifted_keypoints = shuffle(shifted_images, shifted_keypoints, random_state=6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_iY4lUGdnEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "noisy_images, noisy_keypoints = augmentor.addNoise(shifted_images, noise_scale=0.002), shifted_keypoints\n",
        "noisy_images, noisy_keypoints = shuffle(noisy_images, noisy_keypoints, random_state=7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEMXfqsBeNvo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Horizontal:\", h_flipped_images.shape, h_flipped_keypoints.shape)\n",
        "print(\"Vertical:\", v_flipped_images.shape, v_flipped_keypoints.shape)\n",
        "print(\"Rotated:\", rotated_images.shape, rotated_keypoints.shape)\n",
        "print(\"Altered:\", altered_images.shape, altered_keypoints.shape)\n",
        "print(\"Shifted:\", shifted_images.shape, shifted_keypoints.shape)\n",
        "print(\"Noisy:\", noisy_images.shape, noisy_keypoints.shape)\n",
        "print(\"Training Data:\", X_train.shape, y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4dryDu-d5oe",
        "colab_type": "text"
      },
      "source": [
        "**Adding augmented data to training data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOsn2gCDeGg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# new_train_images = np.vstack((X_train, \n",
        "#                               h_flipped_images,\n",
        "#                               rotated_images,\n",
        "#                               altered_images,\n",
        "#                               shifted_images,\n",
        "#                               noisy_images\n",
        "#                               ))\n",
        "\n",
        "# new_train_keypoints = np.vstack((y_train,\n",
        "#                                  h_flipped_keypoints,\n",
        "#                                  rotated_keypoints,\n",
        "#                                  altered_keypoints,\n",
        "#                                  shifted_keypoints,\n",
        "#                                  noisy_keypoints\n",
        "#                                  ))\n",
        "\n",
        "new_train_images = np.vstack((X_train,\n",
        "                              noisy_images[:int(.10*noisy_images.shape[0])]))\n",
        "new_train_keypoints = np.vstack((y_train,\n",
        "                                 noisy_keypoints[:int(.10*noisy_keypoints.shape[0])]))\n",
        "\n",
        "new_train_images, new_train_keypoints = shuffle(new_train_images, new_train_keypoints, random_state=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gnm22XGXgnDX",
        "colab_type": "code",
        "outputId": "3b6c761e-5eaf-4a4b-8f7d-b4a0173ea9c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(new_train_images.shape)\n",
        "print(new_train_keypoints.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4674, 96, 96, 1)\n",
            "(4674, 30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTebQapbmc95",
        "colab_type": "text"
      },
      "source": [
        "#####**Training Model 1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5j9cIqXhFkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model_1()\n",
        "base_learning_rate = 7.5e-5\n",
        "\n",
        "model.compile(\n",
        "    loss = 'mse',\n",
        "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate),\n",
        "    metrics = ['accuracy'] \n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wP6uMTiVm0U_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 500\n",
        "batch_size = 32\n",
        "\n",
        "history = model.fit(\n",
        "    new_train_images,\n",
        "    new_train_keypoints,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSn_txiznSxC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "plt.plot(loss,linewidth=2, label=\"training loss\")\n",
        "plt.plot(val_loss, linewidth=2, label=\"validation loss\")\n",
        "\n",
        "plt.legend()\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"log loss\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(acc, linewidth=2, label='training accuracy')\n",
        "plt.plot(val_acc, linewidth=2, label='validation accuracy')\n",
        "plt.legend()\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibvPNlnnHYvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_learning_rate = 7.5e-7\n",
        "\n",
        "model.compile(\n",
        "    loss = 'mse',\n",
        "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=new_learning_rate),\n",
        "    metrics = ['accuracy'] \n",
        ")\n",
        "\n",
        "more_epochs = 500\n",
        "\n",
        "history = model.fit(\n",
        "    new_train_images,\n",
        "    new_train_keypoints,\n",
        "    epochs=more_epochs+epochs,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    verbose=2,\n",
        "    initial_epoch=history.epoch[-1]+1\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDavWlHaH774",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "plt.plot(loss,linewidth=2, label=\"training loss\")\n",
        "plt.plot(val_loss, linewidth=2, label=\"validation loss\")\n",
        "\n",
        "plt.legend()\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"log loss\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(acc, linewidth=2, label='training accuracy')\n",
        "plt.plot(val_acc, linewidth=2, label='validation accuracy')\n",
        "plt.legend()\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cq0Uz-OH84X",
        "colab_type": "text"
      },
      "source": [
        "**Test Performance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT_nK_0zzBK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test, _ = utils.loadData()\n",
        "y_test = model.predict(X_test)\n",
        "\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.1, wspace=0.1)\n",
        "\n",
        "images = np.random.choice(range(X_test.shape[0]), 16, replace=False)\n",
        "\n",
        "for i in range(16):\n",
        "  axis = fig.add_subplot(5,4, i+1, xticks=[], yticks=[])\n",
        "  utils.plotSample(X_test[images[i]], y_test[images[i]], axis, \"Test Prediction {}\".format(i+1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOpYm3p9T3UU",
        "colab_type": "text"
      },
      "source": [
        "###**Model 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrmwK-enWtQD",
        "colab_type": "text"
      },
      "source": [
        "####**Defining the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjy03_6MUCrW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Model_3():\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2D(filters=32,\n",
        "                                   kernel_size=(3,3),\n",
        "                                   activation='relu',\n",
        "                                   padding='same',\n",
        "                                   input_shape=(96,96,1)\n",
        "                                   ))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2D(filters=32,\n",
        "                                   kernel_size=(3,3),\n",
        "                                   activation='relu',\n",
        "                                   padding='same'\n",
        "                                   ))\n",
        "  model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2D(filters=64,\n",
        "                                   kernel_size=(3,3),\n",
        "                                   activation='relu',\n",
        "                                   padding='same'\n",
        "                                   ))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2D(filters=64,\n",
        "                                   kernel_size=(3,3),\n",
        "                                   activation='relu',\n",
        "                                   padding='same'\n",
        "                                   ))\n",
        "  model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2D(filters=96,\n",
        "                                   kernel_size=(3,3),\n",
        "                                   activation='relu',\n",
        "                                   padding='same'\n",
        "                                   ))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2D(filters=96,\n",
        "                                   kernel_size=(3,3),\n",
        "                                   activation='relu',\n",
        "                                   padding='same'\n",
        "                                   ))\n",
        "  model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2D(filters=128,\n",
        "                                   kernel_size=(3,3),\n",
        "                                   activation='relu',\n",
        "                                   padding='same'\n",
        "                                   ))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2D(filters=128,\n",
        "                                   kernel_size=(3,3),\n",
        "                                   activation='relu',\n",
        "                                   padding='same'\n",
        "                                   ))\n",
        "  model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2D(filters=256,\n",
        "                                   kernel_size=(3,3),\n",
        "                                   activation='relu',\n",
        "                                   padding='same'\n",
        "                                   ))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "  \n",
        "  model.add(tf.keras.layers.Conv2D(filters=256,\n",
        "                                   kernel_size=(3,3),\n",
        "                                   activation='relu',\n",
        "                                   padding='same'\n",
        "                                   ))\n",
        "  model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2D(filters=512,\n",
        "                                   kernel_size=(3,3),\n",
        "                                   activation='relu',\n",
        "                                   padding='same'\n",
        "                                   ))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "  \n",
        "  model.add(tf.keras.layers.Conv2D(filters=512,\n",
        "                                   kernel_size=(3,3),\n",
        "                                   activation='relu',\n",
        "                                   padding='same'\n",
        "                                   ))\n",
        "  model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "  model.add(tf.keras.layers.Dropout(0.1))\n",
        "  model.add(tf.keras.layers.Dense(30))\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5CbDA6IW090",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model_3()\n",
        "base_learning_rate = 2.5e-6\n",
        "\n",
        "model.compile(\n",
        "    loss = 'mse',\n",
        "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate),\n",
        "    metrics = ['mae', 'accuracy'] \n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqLYngjMW9kN",
        "colab_type": "text"
      },
      "source": [
        "**Model Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySkeiEV9W3LD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_model(\n",
        "    model, \n",
        "    to_file='model.png', \n",
        "    show_layer_names=True, \n",
        "    show_shapes=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgl7gN4OX94v",
        "colab_type": "text"
      },
      "source": [
        "####**Training the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aIkKrwXbYB9T",
        "colab": {}
      },
      "source": [
        "epochs = 500 #epochs before the loss starts increasing\n",
        "batch_size = 32\n",
        "\n",
        "history = model.fit(\n",
        "    new_train_images,\n",
        "    new_train_keypoints,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MOYezBzCYB9f"
      },
      "source": [
        "**Learning Rate Check**\n",
        "A check for the optimal curve, monotonically decreasing loss without regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oZ1l0T7iYB9f",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "mae = history.history['mean_absolute_error']\n",
        "val_mae = history.history['val_mean_absolute_error']\n",
        "\n",
        "plt.plot(loss,linewidth=2, label=\"training loss\")\n",
        "plt.plot(val_loss, linewidth=2, label=\"validation loss\")\n",
        "\n",
        "plt.legend()\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"log loss\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(mae, linewidth=2, label='training MAE')\n",
        "plt.plot(val_mae, linewidth=2, label='validation MAE')\n",
        "plt.legend()\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('MAE')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4itK7K0rYB9j"
      },
      "source": [
        "**Continue training with new learning rate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8WTR39xhYB9l",
        "colab": {}
      },
      "source": [
        "more_epochs = 500\n",
        "new_learning_rate = 2.5e-8\n",
        "\n",
        "model.compile(\n",
        "    loss = 'mse',\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=new_learning_rate),\n",
        "    metrics = ['mae','accuracy']\n",
        ")\n",
        "\n",
        "last_epoch = history.epoch[-1] + 1\n",
        "history = model.fit(\n",
        "    new_train_images,\n",
        "    new_train_keypoints,\n",
        "    epochs=more_epochs+last_epoch,\n",
        "    initial_epoch = last_epoch,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    verbose=2\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hlnZ5PcjYB9o"
      },
      "source": [
        "**Learning Rate Check** A check for the optimal curve, monotonically decreasing loss without regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cBfkPArrYB9p",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "mae = history.history['mean_absolute_error']\n",
        "val_mae = history.history['val_mean_absolute_error']\n",
        "\n",
        "plt.plot(loss,linewidth=2, label=\"training loss\")\n",
        "plt.plot(val_loss, linewidth=2, label=\"validation loss\")\n",
        "\n",
        "plt.legend()\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"log loss\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(mae, linewidth=2, label='training MAE')\n",
        "plt.plot(val_mae, linewidth=2, label='validation MAE')\n",
        "plt.legend()\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('MAE')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LD5r4GOczW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test, _ = utils.loadData()\n",
        "y_test = model.predict(X_test)\n",
        "\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.1, wspace=0.1)\n",
        "\n",
        "images = np.random.choice(range(X_test.shape[0]), 16, replace=False)\n",
        "\n",
        "for i in range(16):\n",
        "  axis = fig.add_subplot(5,4, i+1, xticks=[], yticks=[])\n",
        "  utils.plotSample(X_test[images[i]], y_test[images[i]], axis, \"Test Prediction {}\".format(i+1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PxrlAcSBuWC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQAKh4JzJjjG",
        "colab_type": "text"
      },
      "source": [
        "###**Using Missing Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sS1tGzPW6tj",
        "colab_type": "text"
      },
      "source": [
        "#####**Loading Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gOqVSGwSKkJH",
        "colab": {}
      },
      "source": [
        "X_train, y_train = utils.loadData('train', drop_null=False)\n",
        "clean_X_train, clean_y_train = utils.loadData('train', drop_null=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pD4x37znKkJO",
        "colab": {}
      },
      "source": [
        "print (\"Shape of X:\",X_train.shape, \"Min of X:\", X_train.min(), \"Max of X:\", X_train.max())\n",
        "print (\"Shape of y:\",y_train.shape, \"Min of y:\", y_train.min(), \"Max of y:\", y_train.max())\n",
        "print (\"Shape of X:\",clean_X_train.shape, \"Min of X:\", clean_X_train.min(), \"Max of X:\", clean_X_train.max())\n",
        "print (\"Shape of y:\",clean_y_train.shape, \"Min of y:\", clean_y_train.min(), \"Max of y:\", clean_y_train.max())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KL5rqDz6KkJR"
      },
      "source": [
        "**Exploring Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "je0y6xPlKkJS",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(10,10))\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.1, wspace=0.1)\n",
        "\n",
        "images = np.random.choice(range(X_train.shape[0]), 16, replace=False)\n",
        "for i in range(16):\n",
        "  axis = fig.add_subplot(5,4, i+1, xticks=[], yticks=[])\n",
        "  utils.plotSample(X_train[i], y_train[i], axis)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wBDWMc9kXLvx"
      },
      "source": [
        "#####**Preparing Augmented Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s_d6yDYtXLv0",
        "colab": {}
      },
      "source": [
        "augmentor = DataAugmentation.Augmentor()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8bHGyWb-XLv7",
        "colab": {}
      },
      "source": [
        "h_flipped_images, h_flipped_keypoints = augmentor.flip(clean_X_train, clean_y_train)\n",
        "v_flipped_images, v_flipped_keypoints = augmentor.flip(clean_X_train, clean_y_train, False)\n",
        "h_flipped_images, h_flipped_keypoints = shuffle(h_flipped_images, h_flipped_keypoints, random_state=2)\n",
        "v_flipped_images, v_flipped_keypoints = shuffle(v_flipped_images, v_flipped_keypoints, random_state=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HaNcqq3_XLv_",
        "colab": {}
      },
      "source": [
        "rotation_angles = [12]\n",
        "rotated_images, rotated_keypoints = augmentor.rotate(clean_X_train, clean_y_train, rotation_angles)\n",
        "rotated_images, rotated_keypoints = shuffle(rotated_images, rotated_keypoints, random_state=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uDDrDE6KXLwC",
        "colab": {}
      },
      "source": [
        "altered_images, altered_keypoints = augmentor.alterBrightness(clean_X_train, clean_y_train, B=True, D=True)\n",
        "altered_images, altered_keypoints = shuffle(altered_images, altered_keypoints, random_state=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ip7gissqXLwF",
        "colab": {}
      },
      "source": [
        "shifts = [12]\n",
        "shifted_images, shifted_keypoints = augmentor.shiftImage(clean_X_train, clean_y_train, shifts=shifts)\n",
        "shifted_images, shifted_keypoints = shuffle(shifted_images, shifted_keypoints, random_state=6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cTZu1Z6wXLwI",
        "colab": {}
      },
      "source": [
        "noisy_images, noisy_keypoints = augmentor.addNoise(clean_X_train, noise_scale=0.002), clean_y_train\n",
        "noisy_images, noisy_keypoints = shuffle(noisy_images, noisy_keypoints, random_state=7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3azTnAWBXLwL",
        "colab": {}
      },
      "source": [
        "print(\"Horizontal:\", h_flipped_images.shape, h_flipped_keypoints.shape)\n",
        "print(\"Vertical:\", v_flipped_images.shape, v_flipped_keypoints.shape)\n",
        "print(\"Rotated:\", rotated_images.shape, rotated_keypoints.shape)\n",
        "print(\"Altered:\", altered_images.shape, altered_keypoints.shape)\n",
        "print(\"Shifted:\", shifted_images.shape, shifted_keypoints.shape)\n",
        "print(\"Noisy:\", noisy_images.shape, noisy_keypoints.shape)\n",
        "print(\"Training Data:\", X_train.shape, y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2g_UNsl7XLwP"
      },
      "source": [
        "**Adding augmented data to training data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RUxIhoqOXLwQ",
        "colab": {}
      },
      "source": [
        "new_train_images = np.vstack((X_train, \n",
        "                              rotated_images,\n",
        "                              altered_images,\n",
        "                              shifted_images,\n",
        "                              noisy_images\n",
        "                              ))\n",
        "\n",
        "new_train_keypoints = np.vstack((y_train,\n",
        "                                 rotated_keypoints,\n",
        "                                 altered_keypoints,\n",
        "                                 shifted_keypoints,\n",
        "                                 noisy_keypoints\n",
        "                                 ))\n",
        "\n",
        "# new_train_images = np.vstack((X_train,\n",
        "#                               noisy_images[:int(.10*noisy_images.shape[0])]))\n",
        "# new_train_keypoints = np.vstack((y_train,\n",
        "#                                  noisy_keypoints[:int(.10*noisy_keypoints.shape[0])]))\n",
        "\n",
        "new_train_images, new_train_keypoints = shuffle(new_train_images, new_train_keypoints, random_state=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YmiGdGKfXLwS",
        "colab": {}
      },
      "source": [
        "print(new_train_images.shape)\n",
        "print(new_train_keypoints.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76J-KrmYZZx1",
        "colab_type": "text"
      },
      "source": [
        "#####**Training Model 3**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Orke3je6Zf5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpointer = ModelCheckpoint(filepath = 'model_3_best_10px.hdf5', \n",
        "                               monitor='val_mean_absolute_error', \n",
        "                               verbose=1, \n",
        "                               save_best_only=True, \n",
        "                               mode='min')\n",
        "\n",
        "model = Model_3()\n",
        "base_learning_rate = 1e-3\n",
        "batch_size = 32\n",
        "epochs = 100\n",
        "\n",
        "model.compile(\n",
        "    loss = 'mse',\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
        "    metrics = ['mae', 'accuracy'] \n",
        ")\n",
        "\n",
        "history = model.fit(new_train_images,\n",
        "                    new_train_keypoints,\n",
        "                    epochs=epochs,\n",
        "                    batch_size = batch_size,\n",
        "                    callbacks=[checkpointer],\n",
        "                    validation_split = 0.05)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9_aoYKybYYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "mae = history.history['mean_absolute_error']\n",
        "val_mae = history.history['val_mean_absolute_error']\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "plt.plot(loss,linewidth=2, label=\"training loss\")\n",
        "plt.plot(val_loss, linewidth=2, label=\"validation loss\")\n",
        "\n",
        "plt.legend()\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"log loss\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(mae, linewidth=2, label='training MAE')\n",
        "plt.plot(val_mae, linewidth=2, label='validation MAE')\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('MAE')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(acc, linewidth=2, label='training acc')\n",
        "plt.plot(val_acc, linewidth=2, label='validation acc')\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0cwRwhmchDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model_3()\n",
        "model.load_weights('model_3_best_10px.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdXyQt_f0-we",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test, _ = utils.loadData()\n",
        "y_test = model.predict(X_test)\n",
        "\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.1, wspace=0.1)\n",
        "\n",
        "images = np.random.choice(range(X_test.shape[0]), 16, replace=False)\n",
        "\n",
        "for i in range(16):\n",
        "  axis = fig.add_subplot(5,4, i+1, xticks=[], yticks=[])\n",
        "  utils.plotSample(X_test[images[i]], y_test[images[i]], axis, \"Test Prediction {}\".format(i+1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ouo99TxItsbG"
      },
      "source": [
        "#####**Training Model 1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oTLTD2z1tsbJ",
        "colab": {}
      },
      "source": [
        "checkpointer = ModelCheckpoint(filepath = 'model_1_best_15px.hdf5', \n",
        "                               monitor='val_mean_absolute_error', \n",
        "                               verbose=1, \n",
        "                               save_best_only=True, \n",
        "                               mode='min')\n",
        "\n",
        "model = Model_1()\n",
        "base_learning_rate = 1e-3\n",
        "\n",
        "model.compile(\n",
        "    loss = 'mse',\n",
        "    optimizer = tf.keras.optimizers.Adam(),\n",
        "    metrics = ['mae','accuracy'] \n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OMHSVrQvtsbO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "88d0f2c9-3952-43ff-bb2d-3658b8d403e6"
      },
      "source": [
        "epochs = 100\n",
        "batch_size = 32\n",
        "\n",
        "history = model.fit(\n",
        "    new_train_images,\n",
        "    new_train_keypoints,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.05,\n",
        "    callbacks=[checkpointer],\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20576/20606 [============================>.] - ETA: 0s - loss: 0.0033 - mean_absolute_error: 0.0427 - acc: 0.7442\n",
            "Epoch 00011: val_mean_absolute_error improved from 0.03944 to 0.03813, saving model to model_1_best_15px.hdf5\n",
            "20606/20606 [==============================] - 10s 477us/sample - loss: 0.0033 - mean_absolute_error: 0.0427 - acc: 0.7441 - val_loss: 0.0030 - val_mean_absolute_error: 0.0381 - val_acc: 0.7613\n",
            "Epoch 12/100\n",
            "20544/20606 [============================>.] - ETA: 0s - loss: 0.0033 - mean_absolute_error: 0.0427 - acc: 0.7494\n",
            "Epoch 00012: val_mean_absolute_error improved from 0.03813 to 0.03595, saving model to model_1_best_15px.hdf5\n",
            "20606/20606 [==============================] - 10s 471us/sample - loss: 0.0033 - mean_absolute_error: 0.0427 - acc: 0.7492 - val_loss: 0.0027 - val_mean_absolute_error: 0.0360 - val_acc: 0.7908\n",
            "Epoch 13/100\n",
            "20512/20606 [============================>.] - ETA: 0s - loss: 0.0033 - mean_absolute_error: 0.0426 - acc: 0.7515\n",
            "Epoch 00013: val_mean_absolute_error did not improve from 0.03595\n",
            "20606/20606 [==============================] - 10s 468us/sample - loss: 0.0033 - mean_absolute_error: 0.0426 - acc: 0.7513 - val_loss: 0.0189 - val_mean_absolute_error: 0.0872 - val_acc: 0.7097\n",
            "Epoch 14/100\n",
            " 5152/20606 [======>.......................] - ETA: 7s - loss: 0.0035 - mean_absolute_error: 0.0435 - acc: 0.7486"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-6e0502a2b7de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m )\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IQd1SX5XtsbR",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "plt.plot(loss,linewidth=2, label=\"training loss\")\n",
        "plt.plot(val_loss, linewidth=2, label=\"validation loss\")\n",
        "\n",
        "plt.legend()\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"log loss\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(acc, linewidth=2, label='training accuracy')\n",
        "plt.plot(val_acc, linewidth=2, label='validation accuracy')\n",
        "plt.legend()\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu2FWOldvDQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model_1()\n",
        "model.load_weights('model_1_best_15px.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9ROCwJ50tsbb"
      },
      "source": [
        "**Test Performance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wpBGdWJ5tsbc",
        "colab": {}
      },
      "source": [
        "X_test, _ = utils.loadData()\n",
        "y_test = model.predict(X_test)\n",
        "\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.1, wspace=0.1)\n",
        "\n",
        "images = np.random.choice(range(X_test.shape[0]), 16, replace=False)\n",
        "\n",
        "for i in range(16):\n",
        "  axis = fig.add_subplot(5,4, i+1, xticks=[], yticks=[])\n",
        "  utils.plotSample(X_test[images[i]], y_test[images[i]], axis, \"Test Prediction {}\".format(i+1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLsRvQF_4JV1",
        "colab_type": "text"
      },
      "source": [
        "##**Creating Kaggle Submit File**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ0msRVZyJ9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('training.csv')\n",
        "cols = df.columns[:-1]\n",
        "\n",
        "y_pred = []\n",
        "y_test = pd.DataFrame(y_test, columns=cols)\n",
        "y_pred.append(y_test)\n",
        "y_pred = pd.concat(y_pred, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBU_6C_rrDgb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "utils.prepareSubmission(y_pred, 'results')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsLRPmPUyJjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}